{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDeCOtodMCeYqT+VDYuwsL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BautistaBertolami/Cap4630AI/blob/master/HW5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-TX9c7YoB-N",
        "colab_type": "text"
      },
      "source": [
        "**AI class summary Bautista Bertolami**\n",
        "\n",
        "**1) General Concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdaG2uWrItYd",
        "colab_type": "text"
      },
      "source": [
        "**Artificial Intelligence**\n",
        "*   AI deals with simulating a certain amount of human intelligence using a computer, allowing said computer to perform more complicated tasks such as visual recognition, or speech recognition.\n",
        "\n",
        "**Symbolic AI**\n",
        "*   Symbolic AI is the term used to describe AI problems related to symbolic representations of problems such as logic and search.\n",
        "*   Symbolic AI takes in the input and rules and produces an output\n",
        "<pre>\n",
        "Input-------->+---------------+\n",
        "                  |               | -------->Output\n",
        "Rules-------->+---------------+\n",
        "</pre>\n",
        "\n",
        "**Machine Learning**\n",
        "*   Subset of AI\n",
        "*   In ML computers essentially adjust a set of rules by being provided the input and output of a problem. This in turn allows the computer to tailor make rules for a specific problem, this can also allow it to fine tune said rules with more training\n",
        "<pre>\n",
        "Input--------->+---------------+\n",
        "                   |               | -------->Rules\n",
        "Output-------->+---------------+\n",
        "</pre>\n",
        "* Most forms of machine learning fall into a spectrum of supervised and unsupervised learning\n",
        "* Supervised learning\n",
        "  * This model is provided labeled training data, meaning that it had both the input and output for a problem\n",
        "  * Essentially the model creates a set of rules that takes in features(input) and labels them(output)\n",
        "  * An example can be a ML model that labels a dog a specific breed, the label would be the breed, and the features could be the type of tail, length of ears, length of the legs, etc.\n",
        "\n",
        "* Unsupervised learning\n",
        "  * This model is given a set of data, but no labels for said data, here the model must infer it's own way of classifiying the data\n",
        "  * esentially groups data into clusters\n",
        "\n",
        "* Reinforcement learning\n",
        "  * The model is given a reward of sorts for taking a specific action\n",
        "  * The model has to figure out a way to maximize its reward in a certain amount of time or steps \n",
        "\n",
        "* Deep Learning\n",
        "  * A subset of ML focused on aritificial neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED81vctJoGq_",
        "colab_type": "text"
      },
      "source": [
        "**2) Basic concepts**\n",
        "\n",
        "**Regression**\n",
        "* A regression model predicts continuous values, such as the prices of houses depending on the size of the house, location, etc.\n",
        "* It is used to detect or predict a relationship between a label and features\n",
        "\n",
        "**Linear Regression**\n",
        "* Linear regression is a system of regression that can analyze and create/find a relationship between labels and features\n",
        "* Linear regression usually creates a line of best fit to separate data, this is done with the equation $y=mx+b$ which is the simplest version were $x$ is the input for a feature, $y$ is the prediction, $m$ is the associated weight to that specific input, and $b$ is the overall bias.\n",
        "* Linear regression works by taking in the input and output and adjusting the weights of the features and bias to create the most accurate line of best fit possible\n",
        "\n",
        "**Logistic Regression**\n",
        "* Logistic regression is very similar to linear regression except for the fact that it deals with binary classification problems, usually using the sigmoid formula.\n",
        "* The outputs can only be classified as either 0 or 1, nothing in between"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU3tPZSHiFxG",
        "colab_type": "text"
      },
      "source": [
        "**This is an example of weights obtained using logistic regression with two features using binary cross entropy as the loss function (code originally from HW3 Problem 3 originally trained with 80 unit of data)**\n",
        "\n",
        "This example shows the result of using logistical regression to separate data points into two groups, any point above the line is labeled blue(1), and any point below it is labeled red(0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNJ0ZN9QRfPw",
        "colab_type": "code",
        "outputId": "becd1a03-514c-4cb8-852e-178df9f1a2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def display_random_data2(data, labels, w1, w2, b):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
        "  fig = plt.figure(figsize = (5, 5))\n",
        "  axs = plt.axes()\n",
        "  x = [0] * 2\n",
        "  y = [0] * 2\n",
        "  x[0] = 0\n",
        "  x[1] = 1\n",
        "  y[0] = (-(w1 * x[0])-b)/w2\n",
        "  y[1] = (-(w1 * x[1])-b)/w2\n",
        "  plt.plot(x, y, \"magenta\")\n",
        "  for i in range(0, data.shape[0]):\n",
        "    color = \"red\"\n",
        "    if labels[i] == 0:\n",
        "      color = \"blue\"\n",
        "    axs.scatter(data[i][0], data[i][1], c = color)\n",
        "\n",
        "w1 = 1.7078354\n",
        "w2 = -1.01765782\n",
        "b = 1.55928224\n",
        "test_data = np.array([[ 0.73920212,  6.19153818],\n",
        " [ 0.45822348,  3.3948902 ],\n",
        " [ 0.08099486, -2.23494688],\n",
        " [ 0.23137049,  1.27617039],\n",
        " [ 0.00978608,  2.14460088],\n",
        " [ 0.06896728, -0.62826494]])\n",
        "test_labels = np.array([0, 0, 1, 1, 0, 1])\n",
        "display_random_data2(test_data, test_labels, w1, w2, b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_YdNxFrReWW",
        "colab_type": "text"
      },
      "source": [
        "**Gradients**\n",
        "* A ML model is trained by an iterative approach, this means the model when it is trained, it adjusts its weights to minimize the loss (loss is the difference between the prediciton, and the actual label) at each input using a gradient\n",
        "* The gradient of a model depends on the loss function it uses, since it is usually the derivative of the loss function being used\n",
        "**Loss Functions**\n",
        "* This iterative approach uses gradients, these gradients first of all use a loss fucntion to calculate the loss, there's several different loss functions that can be used depending on the problem at hand, such eqautions include but are not limited to\n",
        "* Mean Squared Error\n",
        "$\\frac{1}{m}‎{{‎‎\\sum}}_{i=1}^{m‎}\\frac{1}{2}(x^{(i)^T}w-y{(i)})$\n",
        "  * The derivative is $\\frac{1}{m}{{‎‎\\sum}}_{i=1}^{m‎}x^{(i)}(x^{(i)^T}w-y{(i)})$ (gradient)\n",
        "* Binary cross entropy\n",
        "$-ylog(a)-(1-y)log(1-a)$ (which produces an output between 0 or 1)\n",
        "  * The derivative is $y-a$ (gradient)\n",
        "  * also a goes through the sigmoid function usually $a = \\frac{1}{1+e^{-x}}$\n",
        "\n",
        "**Gradient Descent**\n",
        "* Is used to update the weights and bias of the model by iteratively moving in the direction of steepest decrease in the loss function\n",
        "* Learning rate is a scalar number that multiplies the gradient, this in turn allows for the gradient descent to be adjusted very fast, or very slow, depending on the number of the gradient, .0001 is a slow learning rate, 2.5 is a fast learning rate\n",
        "* There's different ways of handling gradient descent some include\n",
        "  * Stochastic Gradient Descent (uses a batch size of 1 for each iteration)\n",
        "  * Mini-Batch Gradient Descent (uses between 10-100 batches per iteration)\n",
        "  * A batch is a set number of data used to train the model\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6_P5uioG57",
        "colab_type": "text"
      },
      "source": [
        "**3) Building a model**\n",
        "\n",
        "**Layers**\n",
        "* These are the basic building blocks of neural networks\n",
        "* They contain the weights, and one or several tensors learned\n",
        "* There are several different types of layers\n",
        "  * Dense layers, are densely connected, which are usually used to proccess simple vector data, using 2D tensors\n",
        "  * Recurrent layers are used to handle 3D tensors\n",
        "  * 2D Convolutional layers are used to handle 4D tensor data\n",
        "* ReLU is used to apply recitfied linear units to the model \n",
        "\n",
        "**Here's an example of a model being created and layers being added to it using keras**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CObCGldmr9-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28, )))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80hRDeekJ-Hr",
        "colab_type": "text"
      },
      "source": [
        "**Convnet**\n",
        "* Convolutional Neural Networks\n",
        "* A CNN is designed specifically to cassify images\n",
        "* The model takes each pixel of an image an learns how to derive specific information from that\n",
        "* These features for example can be certain outlines in an image, and then those outlines are recognized as specific parts of an item\n",
        "  * For example to recognize a car, the outlines can be separated and then recognized as speicifc parts, such as the wheels, windows, lights, etc.\n",
        "  * Then using fully connected layers and the data the model labels the object as a car, or specific model of car\n",
        "* Convolutions have two parameters\n",
        "  * Size of the tiles extracted\n",
        "  * Depth of the output feature map\n",
        "* Maxpooling\n",
        "  * Is a filter in a CNN that takes the matrix and creates a smaller matrix created from the largest numbers in each chunk of the original matrix\n",
        "* Pre trained convnets can also be used in keras, these can speed up the process when creating a model\n",
        "  * These include\n",
        "  * VGG16\n",
        "  * VGG19\n",
        "  * Xception\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmS7YH3-oHIf",
        "colab_type": "text"
      },
      "source": [
        "**4) Compiling a model**\n",
        "* After the model is built, the next step is to compile it, this usually takes a few steps, unless the model requires severe specifications for this reason several things can be altered on the model when it is to be compiled\n",
        "* Loss function\n",
        "  * Here it's the same as before, the loss function determines the exact number that the model needs to minimize to know it is being more accurate\n",
        "  * In keras it can include Mean Squared Error, Binary Cross Entropy, etc.\n",
        "* Optimizers\n",
        " * Setting this up allows you to specifiy a specific variant of Stochastic Gradient Descent\n",
        " * Learning Rate is specified in the optimizer in keras, however changing it to a specific value is easy, all you do is edit the optimizer you want to use\n",
        "  * also as previously stated the smaller the LR the slower the model will update its weights, but it usually provides more accurate results\n",
        "\n",
        "**Here using the previously established model, here I specify a Learning rate, a loss function, and an optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLGBm8Q9fPnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "# Here the model is compiled using MSE as the loss function, and Stochastic Gradient Descent as the optimizer\n",
        "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "\n",
        "# Here the model is compiled usong MSE and SGD, but with a specific learning rate,\n",
        "# each optimizer however has its own way of specifing the components , this one is for Stochastic Gradient Descent\n",
        "sgd = optimizers.SGD(learning_rate = 0.01)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmjoFBTToHVS",
        "colab_type": "text"
      },
      "source": [
        "**5)Training a model**\n",
        "**Training**\n",
        "* Data\n",
        "  * Training a model requires a set of data that the model can use, this includes a set of data that contains the inputs of the weights, and a set of data that contains the labels for each specific input\n",
        "  * These are also usually split into validation and test data, meaning that you end up with 4 different arrays of data (for supervised learning since you need to provide the labels to the model)\n",
        "    * train_data (the inputs that the model uses to create a prediciton) \n",
        "    * train_labels (the labels that the model has to match its predictions with)\n",
        "    * test_data (same as train_data, but used to validate the accuracy of the model)\n",
        "    * test_labels (same as train_labels, but again used to validate the models accuracy) \n",
        "\n",
        "* The model using the specified data trains itself by adjusting the weights of the model, by comparing inputing its prediction through the specified loss function\n",
        "* The model can also be specified to be trained in a specific sized batch and how many epichs it has to go through to finish training, the more epochs the longer, but possibly more accurate the model can be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyUo75ywlVne",
        "colab_type": "text"
      },
      "source": [
        "**Here's an example of code that would be used to train the previous model we established**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZhI7RKHlUDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data and train_labels are not specified so do not run code\n",
        "model.fit(train_data, train_labels, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfkepdOhs1rs",
        "colab_type": "text"
      },
      "source": [
        "**Validation Sets**\n",
        "* Some problems can be derived through training a model specifically overfitting and underfitting, accuracy is used to fix these.\n",
        "* Overfitting\n",
        "  * This problem means that the model is too good at predicitng the train data, but not any other data\n",
        "  * The model can't generalize its predicitons\n",
        "  * The model is more complex than it should be\n",
        "  * The model can't predict new data well\n",
        "* Underfitting\n",
        "  * Is when the model has not been trained enough\n",
        "  * The model is not accurate\n",
        "  * The model again can't predict new or old data\n",
        "* Validation sets are created to help fix both of these problems\n",
        " * As previously mentioned the data is separated into a test_data, and test_label array, with a validation model you would split these two into two more sets, the validation_data, and validation_labels\n",
        " * These are used to check the model and check if the model is accurate on these tests\n",
        " * The workflow is essentially this\n",
        "  * Pick the model that does best on the validation sets, and then check it once more against the test set\n",
        "  * This in turn prevents more exposure to the full data set making the model more general\n",
        " * There are many different types of validation models, some are\n",
        "  * Hold-out validation\n",
        "    * this one is very basic essentially what I explained before\n",
        "  * K-fold validation\n",
        "    * this one is used when there's little data available\n",
        "    * you partition the data set into several folds (sub arrays of the data)\n",
        "    * in each partition a different fold becomes the test data\n",
        "    * essentially its a giant for loop for training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEibNdr9oLYF",
        "colab_type": "text"
      },
      "source": [
        "**6) Finetuning pre trained models**\n",
        "* The pre trained models we dealt in class were convnets\n",
        "* Fine tuning a convnet\n",
        "  * Pick convnet\n",
        "    * VGG16, Xception, VGG19, etc.\n",
        "  * Figure out naming scheme\n",
        "    * You can probably find the documentation for the names of each layer, however i found it to be easier to just print the layer names with a for loop\n",
        "  * using the layer.name() command\n",
        "\n",
        "**Here's how I obtained the names for the xception layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBNkyZ-eglhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd0d29f8-7ffa-486f-d71f-75204494a87d"
      },
      "source": [
        "from keras.applications import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(150, 150, 3))\n",
        "# warning running this code produces a very long output\n",
        "for layer in conv_base.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n",
            "input_1\n",
            "block1_conv1\n",
            "block1_conv1_bn\n",
            "block1_conv1_act\n",
            "block1_conv2\n",
            "block1_conv2_bn\n",
            "block1_conv2_act\n",
            "block2_sepconv1\n",
            "block2_sepconv1_bn\n",
            "block2_sepconv2_act\n",
            "block2_sepconv2\n",
            "block2_sepconv2_bn\n",
            "conv2d_1\n",
            "block2_pool\n",
            "batch_normalization_1\n",
            "add_1\n",
            "block3_sepconv1_act\n",
            "block3_sepconv1\n",
            "block3_sepconv1_bn\n",
            "block3_sepconv2_act\n",
            "block3_sepconv2\n",
            "block3_sepconv2_bn\n",
            "conv2d_2\n",
            "block3_pool\n",
            "batch_normalization_2\n",
            "add_2\n",
            "block4_sepconv1_act\n",
            "block4_sepconv1\n",
            "block4_sepconv1_bn\n",
            "block4_sepconv2_act\n",
            "block4_sepconv2\n",
            "block4_sepconv2_bn\n",
            "conv2d_3\n",
            "block4_pool\n",
            "batch_normalization_3\n",
            "add_3\n",
            "block5_sepconv1_act\n",
            "block5_sepconv1\n",
            "block5_sepconv1_bn\n",
            "block5_sepconv2_act\n",
            "block5_sepconv2\n",
            "block5_sepconv2_bn\n",
            "block5_sepconv3_act\n",
            "block5_sepconv3\n",
            "block5_sepconv3_bn\n",
            "add_4\n",
            "block6_sepconv1_act\n",
            "block6_sepconv1\n",
            "block6_sepconv1_bn\n",
            "block6_sepconv2_act\n",
            "block6_sepconv2\n",
            "block6_sepconv2_bn\n",
            "block6_sepconv3_act\n",
            "block6_sepconv3\n",
            "block6_sepconv3_bn\n",
            "add_5\n",
            "block7_sepconv1_act\n",
            "block7_sepconv1\n",
            "block7_sepconv1_bn\n",
            "block7_sepconv2_act\n",
            "block7_sepconv2\n",
            "block7_sepconv2_bn\n",
            "block7_sepconv3_act\n",
            "block7_sepconv3\n",
            "block7_sepconv3_bn\n",
            "add_6\n",
            "block8_sepconv1_act\n",
            "block8_sepconv1\n",
            "block8_sepconv1_bn\n",
            "block8_sepconv2_act\n",
            "block8_sepconv2\n",
            "block8_sepconv2_bn\n",
            "block8_sepconv3_act\n",
            "block8_sepconv3\n",
            "block8_sepconv3_bn\n",
            "add_7\n",
            "block9_sepconv1_act\n",
            "block9_sepconv1\n",
            "block9_sepconv1_bn\n",
            "block9_sepconv2_act\n",
            "block9_sepconv2\n",
            "block9_sepconv2_bn\n",
            "block9_sepconv3_act\n",
            "block9_sepconv3\n",
            "block9_sepconv3_bn\n",
            "add_8\n",
            "block10_sepconv1_act\n",
            "block10_sepconv1\n",
            "block10_sepconv1_bn\n",
            "block10_sepconv2_act\n",
            "block10_sepconv2\n",
            "block10_sepconv2_bn\n",
            "block10_sepconv3_act\n",
            "block10_sepconv3\n",
            "block10_sepconv3_bn\n",
            "add_9\n",
            "block11_sepconv1_act\n",
            "block11_sepconv1\n",
            "block11_sepconv1_bn\n",
            "block11_sepconv2_act\n",
            "block11_sepconv2\n",
            "block11_sepconv2_bn\n",
            "block11_sepconv3_act\n",
            "block11_sepconv3\n",
            "block11_sepconv3_bn\n",
            "add_10\n",
            "block12_sepconv1_act\n",
            "block12_sepconv1\n",
            "block12_sepconv1_bn\n",
            "block12_sepconv2_act\n",
            "block12_sepconv2\n",
            "block12_sepconv2_bn\n",
            "block12_sepconv3_act\n",
            "block12_sepconv3\n",
            "block12_sepconv3_bn\n",
            "add_11\n",
            "block13_sepconv1_act\n",
            "block13_sepconv1\n",
            "block13_sepconv1_bn\n",
            "block13_sepconv2_act\n",
            "block13_sepconv2\n",
            "block13_sepconv2_bn\n",
            "conv2d_4\n",
            "block13_pool\n",
            "batch_normalization_4\n",
            "add_12\n",
            "block14_sepconv1\n",
            "block14_sepconv1_bn\n",
            "block14_sepconv1_act\n",
            "block14_sepconv2\n",
            "block14_sepconv2_bn\n",
            "block14_sepconv2_act\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOxOw1XYLU75",
        "colab_type": "text"
      },
      "source": [
        "  * Now that we have the layer names we can proceed with the next step\n",
        "  * Freezing Layers\n",
        "    * To fine tune a convnet, you have to freeze all the layers in the convnet and test to see how accurate the model is, if it is not accurate enough you proceed with the actual fine-tuning\n",
        "  * Unfreezing layers\n",
        "    * To fine tune a convnet you have to unfreeze an amount of layers of your choice\n",
        "    * Unfreezing the layers allows the model to change the convnet to better fit the model you are using"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W283Yt5MN0fa",
        "colab_type": "text"
      },
      "source": [
        "**Here's an example of freezing the layers, and then unfreezing a specific amount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeOAK2CBNzoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freezing the layers\n",
        "conv_base.trainable = False\n",
        "\n",
        "# Unfreezing the layers\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block3_pool': # any layer after block3_pool will be trainable\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}